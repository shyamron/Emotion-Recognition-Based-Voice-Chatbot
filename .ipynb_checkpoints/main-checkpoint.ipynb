{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bf7c57b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import librosa\n",
    "import os\n",
    "import librosa.display\n",
    "import re\n",
    "from keras.models import load_model\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import speech_recognition as sr\n",
    "import pyttsx3 \n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070d0519",
   "metadata": {},
   "source": [
    "# EMOTION DETECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0d03dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained():\n",
    "    model1 = load_model('emotion_model\\Emotion_Model.h5')\n",
    "    model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    Sentiments = {  0 : \"female_angry\",\n",
    "                  11 : \"male_sad\",\n",
    "                   1 : \"female_disgust\",\n",
    "                   2 : \"female_fear\",\n",
    "                   3 : \"female_happy\",\n",
    "                   4 : \"female_neutral\",\n",
    "                   5 : \"female_sad\",\n",
    "                   6 : \"male_angry\",\n",
    "                   7 : \"male_disgust\",\n",
    "                   8 : \"male_fear\",\n",
    "                  9: \"male_happy\",\n",
    "                  10 :\"male_neutral\"\n",
    "    }\n",
    "    return model1,Sentiments\n",
    "\n",
    "def extract_feature(filename):\n",
    "    X, sample_rate = librosa.load(filename,sr=44100,offset=0.5)\n",
    "    mfcc=np.mean(librosa.feature.mfcc(y=X,sr=sample_rate,n_mfcc=40).T,axis=0)\n",
    "#     print(0)\n",
    "    return mfcc\n",
    "\n",
    "def speech_emotion(livepreds):\n",
    "    ave={}\n",
    "    for i, prob in enumerate(livepreds[0]):\n",
    "        if i==4 or i==10:\n",
    "            if 'neutral' not in ave:\n",
    "                ave['neutral'] = []\n",
    "            ave['neutral'].append(prob)\n",
    "        if i==0 or i==6:\n",
    "            if 'angry' not in ave:\n",
    "                ave['angry'] = []\n",
    "            ave['angry'].append(prob)\n",
    "        if i==11 or i==5:\n",
    "            if 'sad' not in ave:\n",
    "                ave['sad'] = []\n",
    "            ave['sad'].append(prob)\n",
    "        if i==1 or i==7:\n",
    "            if 'disgust' not in ave:\n",
    "                ave['disgust'] = []\n",
    "            ave['disgust'].append(prob)\n",
    "        if i==2 or i==8:\n",
    "            if 'fear' not in ave:\n",
    "                ave['fear'] = []\n",
    "            ave['fear'].append(prob)\n",
    "        if i==3 or i==9:\n",
    "            if 'happy' not in ave:\n",
    "                ave['happy'] = []\n",
    "            ave['happy'].append(prob)\n",
    "\n",
    "    for key,value in ave.items():\n",
    "        ave[key]=max(value)\n",
    "    return ave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6951ad97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_emotion(MyText):\n",
    "    test=pd.read_csv('emotion_model//x_test.csv').squeeze()\n",
    "    test.loc[len(test)] = MyText\n",
    "    text_model=joblib.load('emotion_model//Text_emotion.sav')\n",
    "    #Get Probabilities\n",
    "    sample_prob = text_model.predict_proba(test)[-1]\n",
    "    # print(type(sample_prob))\n",
    "    emotions=['sad','angry','fear','happy','neutral','disgust']\n",
    "    dic={}\n",
    "    for i, val in enumerate(sample_prob):\n",
    "        dic[emotions[i]] = val\n",
    "    return dic\n",
    "\n",
    "def getemotion(audio,MyText):\n",
    "    model,Sentiments=pretrained()\n",
    "    livedf2= extract_feature(audio)\n",
    "    livedf2= pd.DataFrame(data=livedf2)\n",
    "    # print(livedf2.shape)\n",
    "    \n",
    "    if len(livedf2)<40:\n",
    "        l=40-len(df)\n",
    "        for i in range(l):\n",
    "            livedf2.loc[len(livedf2.index)] = [0]\n",
    "    livedf2 = livedf2.stack().to_frame().T\n",
    "    twodim= np.expand_dims(livedf2, axis=2)\n",
    "\n",
    "    livepreds = model.predict(twodim, batch_size=32, verbose=1)\n",
    "\n",
    "    speech_emotion_preds=speech_emotion(livepreds)\n",
    "    text_emotion_preds=text_emotion(MyText)\n",
    "\n",
    "    # print(\"**text_emotion_preds:\",text_emotion_preds)\n",
    "    # print(\"**speech_emotion_preds:\",speech_emotion_preds)\n",
    "\n",
    "    final_dic={}\n",
    "    for key in speech_emotion_preds:\n",
    "        for key in text_emotion_preds:\n",
    "            final_dic[key] =((speech_emotion_preds[key]*0.4) + (text_emotion_preds[key]*0.6))\n",
    "    \n",
    "    emotion=max(final_dic,key=final_dic.get)\n",
    "    return emotion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63b3897",
   "metadata": {},
   "source": [
    "# Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cc2114c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(x):\n",
    "  seq_len = tf.shape(x)[1]\n",
    "  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "  padding_mask = create_padding_mask(x)\n",
    "  return tf.maximum(look_ahead_mask, padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40ba5b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(x):\n",
    "  mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "  # (batch_size, 1, 1, sequence length)\n",
    "  return mask[:, tf.newaxis, tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5e65b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, position, d_model):\n",
    "    super(PositionalEncoding, self).__init__()\n",
    "    self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "  def get_angles(self, position, i, d_model):\n",
    "    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "    return position * angles\n",
    "\n",
    "  def positional_encoding(self, position, d_model):\n",
    "    angle_rads = self.get_angles(\n",
    "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "        d_model=d_model)\n",
    "    # apply sin to even index in the array\n",
    "    sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "    # apply cos to odd index in the array\n",
    "    cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "    pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b8a6141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "  \"\"\"Calculate the attention weights. \"\"\"\n",
    "  matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "  # scale matmul_qk\n",
    "  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "  logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "  # add the mask to zero out padding tokens\n",
    "  if mask is not None:\n",
    "    logits += (mask * -1e9)\n",
    "\n",
    "  # softmax is normalized on the last axis (seq_len_k)\n",
    "  attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "  output = tf.matmul(attention_weights, value)\n",
    "\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c99087f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "    super(MultiHeadAttention, self).__init__(name=name)\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "\n",
    "    assert d_model % self.num_heads == 0\n",
    "\n",
    "    self.depth = d_model // self.num_heads\n",
    "\n",
    "    self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "  def split_heads(self, inputs, batch_size):\n",
    "    inputs = tf.reshape(\n",
    "        inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "  def call(self, inputs):\n",
    "    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "        'value'], inputs['mask']\n",
    "    batch_size = tf.shape(query)[0]\n",
    "\n",
    "    # linear layers\n",
    "    query = self.query_dense(query)\n",
    "    key = self.key_dense(key)\n",
    "    value = self.value_dense(value)\n",
    "\n",
    "    # split heads\n",
    "    query = self.split_heads(query, batch_size)\n",
    "    key = self.split_heads(key, batch_size)\n",
    "    value = self.split_heads(value, batch_size)\n",
    "\n",
    "    # scaled dot-product attention\n",
    "    scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "    # concatenation of heads\n",
    "    concat_attention = tf.reshape(scaled_attention,\n",
    "                                  (batch_size, -1, self.d_model))\n",
    "\n",
    "    # final linear layer\n",
    "    outputs = self.dense(concat_attention)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49cfb4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name=\"encoder\"):\n",
    "  input1 = tf.keras.Input(shape=(None,), name=\"input1\")\n",
    "  input2 = tf.keras.Input(shape=(None,), name=\"imput2\")\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  embedding1 = tf.keras.layers.Embedding(vocab_size, d_model)(input1)\n",
    "  embedding1 *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "  embedding2 = tf.keras.layers.Embedding(vocab_size, d_model)(input2)\n",
    "  embedding2 *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "  embeddings = outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(embedding1 + embedding2)\n",
    "\n",
    "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  for i in range(num_layers):\n",
    "    outputs = encoder_layer(\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        name=\"encoder_layer_{}\".format(i),\n",
    "    )([outputs, padding_mask])\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[input1,input2, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50165a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  attention = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention\")({\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "  attention = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention + outputs)\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d49cfc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name='decoder'):\n",
    "  inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "  enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "  look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name='look_ahead_mask')\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "  \n",
    "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  for i in range(num_layers):\n",
    "    outputs = decoder_layer(\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        name='decoder_layer_{}'.format(i),\n",
    "    )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7cb677b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "  enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "  look_ahead_mask = tf.keras.Input(shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "  attention1 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': look_ahead_mask\n",
    "      })\n",
    "  attention1 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "  attention2 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "          'query': attention1,\n",
    "          'key': enc_outputs,\n",
    "          'value': enc_outputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "  attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "  attention2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention2 + attention1)\n",
    "\n",
    "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(outputs + attention2)\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87887492",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7db4dfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  \n",
    "  loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "      from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "  mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "  loss = tf.multiply(loss, mask)\n",
    "\n",
    "  return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73b57fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(vocab_size,\n",
    "                num_layers,\n",
    "                units,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dropout,\n",
    "                name=\"transformer\"):\n",
    "  input1 = tf.keras.Input(shape=(None,), name=\"input1\")\n",
    "  input2 = tf.keras.Input(shape=(None,), name=\"input2\")\n",
    "  dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "  enc_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='enc_padding_mask')(input1)\n",
    "  # mask the future tokens for decoder inputs at the 1st attention block\n",
    "  look_ahead_mask = tf.keras.layers.Lambda(\n",
    "      create_look_ahead_mask,\n",
    "      output_shape=(1, None, None),\n",
    "      name='look_ahead_mask')(dec_inputs)\n",
    "  # mask the encoder outputs for the 2nd attention block\n",
    "  dec_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='dec_padding_mask')(input1)\n",
    "\n",
    "  enc_outputs = encoder(\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "  )(inputs=[input1, input2, enc_padding_mask])\n",
    "\n",
    "  dec_outputs = decoder(\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "  )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "  outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "\n",
    "  return tf.keras.Model(inputs=[input1, input2, dec_inputs], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25baa9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(optimizer):\n",
    "    NUM_LAYERS = 2\n",
    "    D_MODEL = 256\n",
    "    NUM_HEADS = 8\n",
    "    UNITS = 512\n",
    "    DROPOUT = 0.1\n",
    "\n",
    "    model = transformer(\n",
    "    vocab_size=8394,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "    model.compile(optimizer=optimizer, loss=loss_function)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4404116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ffdfa532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test,sentence,emotion,tokenizer, START_TOKEN,END_TOKEN,VOCAB_SIZE):\n",
    "    MAX_LENGTH=40\n",
    "    \n",
    "    \n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    sentence = tf.expand_dims(\n",
    "        START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0\n",
    "        \n",
    "    )\n",
    "    emotion = tokenizer.encode(emotion)\n",
    "\n",
    "    output = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "    for i in range(MAX_LENGTH):\n",
    "        predictions = test(inputs=[sentence,emotion,output], training=False)\n",
    "\n",
    "        predictions = predictions[:, -1:, :]\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "            break\n",
    "\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output, axis=0)\n",
    "\n",
    "\n",
    "def predict(test,sentence,emotion,tokenizer, START_TOKEN,END_TOKEN,VOCAB_SIZE):\n",
    "    prediction = evaluate(test,sentence,emotion,tokenizer, START_TOKEN,END_TOKEN,VOCAB_SIZE)\n",
    "    predicted_sentence = tokenizer.decode(\n",
    "        [i for i in prediction if i < tokenizer.vocab_size]\n",
    "    )\n",
    "    return predicted_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8bf6edf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def buildtoken():\n",
    "    # Load tokenizer\n",
    "    tokenizer = tfds.deprecated.text.SubwordTextEncoder.load_from_file('tokenizer.tf')\n",
    "\n",
    "    # Define start and end token to indicate the start and end of a sentence\n",
    "    START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "\n",
    "    # Vocabulary size plus start and end token\n",
    "    VOCAB_SIZE = tokenizer.vocab_size + 2\n",
    "    return tokenizer, START_TOKEN,END_TOKEN,VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc82ff64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize():\n",
    "    learning_rate = CustomSchedule(256)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30f57d06",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def main(tokenizer, START_TOKEN,END_TOKEN,VOCAB_SIZE):\n",
    "    r = sr.Recognizer() \n",
    "    while(1):\n",
    "        print('Please Speak')\n",
    "        try:\n",
    "            with sr.Microphone() as source2:\n",
    "                r.adjust_for_ambient_noise(source2, duration=0.2)\n",
    "                audio2 = r.listen(source2)\n",
    "                MyText = r.recognize_google(audio2)\n",
    "                MyText = MyText.lower()\n",
    "\n",
    "                print ('****MyText****:', MyText)\n",
    "\n",
    "                with open(\"microphone-results.wav\", \"wb\") as f:\n",
    "                    f.write(audio2.get_wav_data())\n",
    "                \n",
    "                #GETTING EMOTIION\n",
    "                audio='microphone-results.wav'\n",
    "                emotion=getemotion(audio,MyText)\n",
    "                \n",
    "                #GETTING REPLY\n",
    "                optimizer=optimize()\n",
    "                test=create_model(optimizer)\n",
    "                test.load_weights('mymodel.h5')\n",
    "                print(predict(test,MyText,emotion,tokenizer, START_TOKEN,END_TOKEN,VOCAB_SIZE))\n",
    "                \n",
    "        except sr.RequestError as e:\n",
    "            print(\"Could not request results; {0}\".format(e)+'please speak again.')\n",
    "\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Unknown error occured,please speak again..\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "072f9689",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer, START_TOKEN,END_TOKEN,VOCAB_SIZE=buildtoken()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fd859290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please Speak\n",
      "****MyText****: hello\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000137E8352290> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 124ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dongo\\anaconda\\envs\\transformer\\lib\\site-packages\\sklearn\\base.py:299: UserWarning: Trying to unpickle estimator TfidfTransformer from version 1.2.0 when using version 1.2.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\dongo\\anaconda\\envs\\transformer\\lib\\site-packages\\sklearn\\base.py:299: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 1.2.0 when using version 1.2.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\dongo\\anaconda\\envs\\transformer\\lib\\site-packages\\sklearn\\base.py:299: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.2.0 when using version 1.2.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\dongo\\anaconda\\envs\\transformer\\lib\\site-packages\\sklearn\\base.py:299: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 1.2.0 when using version 1.2.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\dongo\\anaconda\\envs\\transformer\\lib\\site-packages\\sklearn\\base.py:299: UserWarning: Trying to unpickle estimator Pipeline from version 1.2.0 when using version 1.2.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi , how are you ?\n",
      "Please Speak\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [38]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSTART_TOKEN\u001b[49m\u001b[43m,\u001b[49m\u001b[43mEND_TOKEN\u001b[49m\u001b[43m,\u001b[49m\u001b[43mVOCAB_SIZE\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [23]\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(tokenizer, START_TOKEN, END_TOKEN, VOCAB_SIZE)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sr\u001b[38;5;241m.\u001b[39mMicrophone() \u001b[38;5;28;01mas\u001b[39;00m source2:\n\u001b[0;32m      7\u001b[0m     r\u001b[38;5;241m.\u001b[39madjust_for_ambient_noise(source2, duration\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m     audio2 \u001b[38;5;241m=\u001b[39m \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlisten\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     MyText \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mrecognize_google(audio2)\n\u001b[0;32m     10\u001b[0m     MyText \u001b[38;5;241m=\u001b[39m MyText\u001b[38;5;241m.\u001b[39mlower()\n",
      "File \u001b[1;32m~\\anaconda\\envs\\transformer\\lib\\site-packages\\speech_recognition\\__init__.py:652\u001b[0m, in \u001b[0;36mRecognizer.listen\u001b[1;34m(self, source, timeout, phrase_time_limit, snowboy_configuration)\u001b[0m\n\u001b[0;32m    649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m phrase_time_limit \u001b[38;5;129;01mand\u001b[39;00m elapsed_time \u001b[38;5;241m-\u001b[39m phrase_start_time \u001b[38;5;241m>\u001b[39m phrase_time_limit:\n\u001b[0;32m    650\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 652\u001b[0m buffer \u001b[38;5;241m=\u001b[39m \u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCHUNK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# reached end of the stream\u001b[39;00m\n\u001b[0;32m    654\u001b[0m frames\u001b[38;5;241m.\u001b[39mappend(buffer)\n",
      "File \u001b[1;32m~\\anaconda\\envs\\transformer\\lib\\site-packages\\speech_recognition\\__init__.py:161\u001b[0m, in \u001b[0;36mMicrophone.MicrophoneStream.read\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\u001b[38;5;28mself\u001b[39m, size):\n\u001b[1;32m--> 161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpyaudio_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexception_on_overflow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda\\envs\\transformer\\lib\\site-packages\\pyaudio.py:612\u001b[0m, in \u001b[0;36mStream.read\u001b[1;34m(self, num_frames, exception_on_overflow)\u001b[0m\n\u001b[0;32m    608\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_input:\n\u001b[0;32m    609\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot input stream\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    610\u001b[0m                   paCanNotReadFromAnOutputOnlyStream)\n\u001b[1;32m--> 612\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexception_on_overflow\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main(tokenizer, START_TOKEN,END_TOKEN,VOCAB_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7162ce58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5221358b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02adc7dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
